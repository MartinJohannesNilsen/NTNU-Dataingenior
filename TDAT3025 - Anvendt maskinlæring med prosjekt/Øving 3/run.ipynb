{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599576429795",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy in epoch 1: 97.05999755859375%\nAccuracy in epoch 2: 98.11000061035156%\nAccuracy in epoch 3: 98.32999420166016%\nAccuracy in epoch 4: 98.43999481201172%\nAccuracy in epoch 5: 98.6199951171875%\nAccuracy in epoch 6: 98.63999938964844%\nAccuracy in epoch 7: 98.22000122070312%\nAccuracy in epoch 8: 98.30999755859375%\nAccuracy in epoch 9: 98.36000061035156%\nAccuracy in epoch 10: 98.40999603271484%\nAccuracy in epoch 11: 98.38999938964844%\nAccuracy in epoch 12: 98.73999786376953%\nAccuracy in epoch 13: 98.69999694824219%\nAccuracy in epoch 14: 98.54999542236328%\nAccuracy in epoch 15: 98.58999633789062%\nAccuracy in epoch 16: 98.23999786376953%\nAccuracy in epoch 17: 98.72999572753906%\nAccuracy in epoch 18: 98.54999542236328%\nAccuracy in epoch 19: 98.43999481201172%\nAccuracy in epoch 20: 98.65999603271484%\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The task given was to create the model in the Model.png, with 2 layers of convolution and pooling, and one layer of dense (fully-connected layer)\n",
    "I use the nn.py from our lecturer as a base, and works mostly inside the class, within the init and logits-method\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# Load observations from the mnist dataset. The observations are divided into a training set and a test set\n",
    "mnist_train = torchvision.datasets.MNIST('./data', train=True, download=True)\n",
    "# torch.functional.nn.conv2d argument must include channels (1)\n",
    "x_train = mnist_train.data.reshape(-1, 1, 28, 28).float()\n",
    "# Create output tensor\n",
    "y_train = torch.zeros((mnist_train.targets.shape[0], 10))\n",
    "y_train[torch.arange(mnist_train.targets.shape[0]),\n",
    "        mnist_train.targets] = 1  # Populate output\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST('./data', train=False, download=True)\n",
    "# torch.functional.nn.conv2d argument must include channels (1)\n",
    "x_test = mnist_test.data.reshape(-1, 1, 28, 28).float()\n",
    "y_test = torch.zeros((mnist_test.targets.shape[0], 10))  # Create output tensor\n",
    "y_test[torch.arange(mnist_test.targets.shape[0]),\n",
    "       mnist_test.targets] = 1  # Populate output\n",
    "\n",
    "# Normalization of inputs\n",
    "mean = x_train.mean()\n",
    "std = x_train.std()\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "# Divide training data into batches to speed up optimization\n",
    "batches = 600\n",
    "x_train_batches = torch.split(x_train, batches)\n",
    "y_train_batches = torch.split(y_train, batches)\n",
    "\n",
    "\n",
    "class ConvolutionalNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNeuralNetworkModel, self).__init__()\n",
    "\n",
    "        # Initialises the Model layers (includes initialized model variables):\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv_first = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.conv_second = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.dense = nn.Linear(64 * 7 * 7, 10)\n",
    "\n",
    "    def logits(self, x):\n",
    "        # Here is the model layers in the right order\n",
    "        x = self.pool(self.conv_first(x)) # First convolutional layer and pooling\n",
    "        x = self.pool(self.conv_second(x)) # Second convolutional layer and pooling\n",
    "        return self.dense(x.view(-1, 64 * 7 * 7))\n",
    "\n",
    "    # Predictor\n",
    "    def f(self, x):\n",
    "        return torch.softmax(self.logits(x), dim=1)\n",
    "\n",
    "    # Cross Entropy loss\n",
    "    def loss(self, x, y):\n",
    "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))\n",
    "\n",
    "    # Accuracy\n",
    "    def accuracy(self, x, y):\n",
    "        return torch.mean(torch.eq(self.f(x).argmax(1), y.argmax(1)).float())\n",
    "\n",
    "\n",
    "model = ConvolutionalNeuralNetworkModel()\n",
    "\n",
    "epoch = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Optimize: adjust W and b to minimize loss using stochastic gradient descent\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for i, epoch in enumerate(range(epoch)):\n",
    "    for batch in range(len(x_train_batches)):\n",
    "        # Compute loss gradients\n",
    "        model.loss(x_train_batches[batch], y_train_batches[batch]).backward()\n",
    "        optimizer.step()  # Perform optimization by adjusting W and b,\n",
    "        optimizer.zero_grad()  # Clear gradients for next step\n",
    "\n",
    "    print(f'Accuracy in epoch {i+1}: {model.accuracy(x_test, y_test)*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy in epoch 1: 97.65999603271484%\nAccuracy in epoch 2: 98.11000061035156%\nAccuracy in epoch 3: 98.3699951171875%\nAccuracy in epoch 4: 98.12999725341797%\nAccuracy in epoch 5: 98.07999420166016%\nAccuracy in epoch 6: 97.8499984741211%\nAccuracy in epoch 7: 98.08999633789062%\nAccuracy in epoch 8: 97.88999938964844%\nAccuracy in epoch 9: 97.93000030517578%\nAccuracy in epoch 10: 97.89999389648438%\nAccuracy in epoch 11: 97.43000030517578%\nAccuracy in epoch 12: 98.1500015258789%\nAccuracy in epoch 13: 98.13999938964844%\nAccuracy in epoch 14: 97.68999481201172%\nAccuracy in epoch 15: 98.30999755859375%\nAccuracy in epoch 16: 98.08999633789062%\nAccuracy in epoch 17: 97.7199935913086%\nAccuracy in epoch 18: 97.7699966430664%\nAccuracy in epoch 19: 98.05999755859375%\nAccuracy in epoch 20: 98.22000122070312%\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The task given was to create the model in the Model.png, with 2 layers of convolution and pooling, and two layers of dense (fully-connected layer)\n",
    "I use the nn.py from our lecturer as a base, and works mostly inside the class, within the init and logits-method\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# Load observations from the mnist dataset. The observations are divided into a training set and a test set\n",
    "mnist_train = torchvision.datasets.MNIST('./data', train=True, download=True)\n",
    "# torch.functional.nn.conv2d argument must include channels (1)\n",
    "x_train = mnist_train.data.reshape(-1, 1, 28, 28).float()\n",
    "# Create output tensor\n",
    "y_train = torch.zeros((mnist_train.targets.shape[0], 10))\n",
    "y_train[torch.arange(mnist_train.targets.shape[0]),\n",
    "        mnist_train.targets] = 1  # Populate output\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST('./data', train=False, download=True)\n",
    "# torch.functional.nn.conv2d argument must include channels (1)\n",
    "x_test = mnist_test.data.reshape(-1, 1, 28, 28).float()\n",
    "y_test = torch.zeros((mnist_test.targets.shape[0], 10))  # Create output tensor\n",
    "y_test[torch.arange(mnist_test.targets.shape[0]),\n",
    "       mnist_test.targets] = 1  # Populate output\n",
    "\n",
    "# Normalization of inputs\n",
    "mean = x_train.mean()\n",
    "std = x_train.std()\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "# Divide training data into batches to speed up optimization\n",
    "batches = 600\n",
    "x_train_batches = torch.split(x_train, batches)\n",
    "y_train_batches = torch.split(y_train, batches)\n",
    "\n",
    "\"\"\"\n",
    "Except for the printing, the only place we change the model is in the logitsmethod, and define new methods in init\n",
    "\"\"\"\n",
    "class ConvolutionalNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNeuralNetworkModel, self).__init__()\n",
    "\n",
    "        # Initialises the Model layers (includes initialized model variables):\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv_first = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.conv_second = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.dense_first = nn.Linear(64 * 7 * 7, 1024) # Other sources call these fc1 and fc2\n",
    "        self.dense_second = nn.Linear(1024, 10)\n",
    "\n",
    "    def logits(self, x):\n",
    "        # Here is the model layers in the right order\n",
    "        x = self.pool(self.conv_first(x)) # First convolutional layer and pooling\n",
    "        x = self.pool(self.conv_second(x)) # Second convolutional layer and pooling\n",
    "        x = self.dense_first(x.view(-1, 64 * 7 * 7))\n",
    "        x = self.dense_second(x.view(-1, 1024)) # Two dense layers\n",
    "        return x\n",
    "\n",
    "    # Predictor\n",
    "    def f(self, x):\n",
    "        return torch.softmax(self.logits(x), dim=1)\n",
    "\n",
    "    # Cross Entropy loss\n",
    "    def loss(self, x, y):\n",
    "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))\n",
    "\n",
    "    # Accuracy\n",
    "    def accuracy(self, x, y):\n",
    "        return torch.mean(torch.eq(self.f(x).argmax(1), y.argmax(1)).float())\n",
    "\n",
    "\n",
    "model = ConvolutionalNeuralNetworkModel()\n",
    "\n",
    "epoch = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Optimize: adjust W and b to minimize loss using stochastic gradient descent\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for i, epoch in enumerate(range(epoch)):\n",
    "    for batch in range(len(x_train_batches)):\n",
    "        # Compute loss gradients\n",
    "        model.loss(x_train_batches[batch], y_train_batches[batch]).backward()\n",
    "        optimizer.step()  # Perform optimization by adjusting W and b,\n",
    "        optimizer.zero_grad()  # Clear gradients for next step\n",
    "\n",
    "    print(f'Accuracy in epoch {i+1}: {model.accuracy(x_test, y_test)*100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy in epoch 1: 98.11000061035156%\nAccuracy in epoch 2: 98.48999786376953%\nAccuracy in epoch 3: 98.76000213623047%\nAccuracy in epoch 4: 98.5%\nAccuracy in epoch 5: 98.5199966430664%\nAccuracy in epoch 6: 98.7699966430664%\nAccuracy in epoch 7: 98.91999816894531%\nAccuracy in epoch 8: 98.69999694824219%\nAccuracy in epoch 9: 98.43000030517578%\nAccuracy in epoch 10: 98.86000061035156%\nAccuracy in epoch 11: 98.22000122070312%\nAccuracy in epoch 12: 99.08000183105469%\nAccuracy in epoch 13: 98.41999816894531%\nAccuracy in epoch 14: 98.6199951171875%\nAccuracy in epoch 15: 98.7699966430664%\nAccuracy in epoch 16: 98.77999877929688%\nAccuracy in epoch 17: 98.52999877929688%\nAccuracy in epoch 18: 98.97999572753906%\nAccuracy in epoch 19: 98.8699951171875%\nAccuracy in epoch 20: 98.91999816894531%\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The task given was to improve the model using for example ReLU or dropout\n",
    "I use the nn.py from our lecturer as a base, and works mostly inside the class, within the init and logits-method\n",
    "\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load observations from the mnist dataset. The observations are divided into a training set and a test set\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    './data', train=True, download=True)\n",
    "# torch.functional.nn.conv2d argument must include channels (1)\n",
    "x_train = mnist_train.data.reshape(-1, 1, 28, 28).float()\n",
    "# Create output tensor\n",
    "y_train = torch.zeros((mnist_train.targets.shape[0], 10))\n",
    "y_train[torch.arange(mnist_train.targets.shape[0]),\n",
    "        mnist_train.targets] = 1  # Populate output\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    './data', train=False, download=True)\n",
    "# torch.functional.nn.conv2d argument must include channels (1)\n",
    "x_test = mnist_test.data.reshape(-1, 1, 28, 28).float()\n",
    "y_test = torch.zeros((mnist_test.targets.shape[0], 10))  # Create output tensor\n",
    "y_test[torch.arange(mnist_test.targets.shape[0]),\n",
    "       mnist_test.targets] = 1  # Populate output\n",
    "\n",
    "# Normalization of inputs\n",
    "mean = x_train.mean()\n",
    "std = x_train.std()\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "# Divide training data into batches to speed up optimization\n",
    "batches = 600\n",
    "x_train_batches = torch.split(x_train, batches)\n",
    "y_train_batches = torch.split(y_train, batches)\n",
    "\n",
    "\n",
    "class ConvolutionalNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNeuralNetworkModel, self).__init__()\n",
    "        # Initialises the Model layers (includes initialized model variables):\n",
    "        # Could also write stride=2 here, but I suspect it is the standard for kernelsize 2 (as 2x2 will jump 2 to the right for the next 2x2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool(self.conv1(x))\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.dropout(F.relu(self.conv1(x)), p=0.1))\n",
    "        \n",
    "        # x = self.pool(self.conv2(x))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.dropout(F.relu(self.conv2(x)), p=0.1))\n",
    "        \n",
    "        x = self.fc1(x.view(-1, 64 * 7 * 7))\n",
    "        x = self.fc2(x.view(-1, 1024))\n",
    "        x = self.fc3(x.view(-1, 100))\n",
    "        return x\n",
    "\n",
    "    # Predictor\n",
    "    def f(self, x):\n",
    "        return torch.softmax(self.forward(x), dim=1)\n",
    "\n",
    "    # Cross Entropy loss\n",
    "    def loss(self, x, y):\n",
    "        return nn.functional.cross_entropy(self.forward(x), y.argmax(1))\n",
    "\n",
    "    # Accuracy\n",
    "    def accuracy(self, x, y):\n",
    "        return torch.mean(torch.eq(self.f(x).argmax(1), y.argmax(1)).float())\n",
    "\n",
    "\n",
    "model = ConvolutionalNeuralNetworkModel()\n",
    "\n",
    "epoch = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Optimize: adjust W and b to minimize loss using stochastic gradient descent\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for i, epoch in enumerate(range(epoch)):\n",
    "    for batch in range(len(x_train_batches)):\n",
    "        # Compute loss gradients\n",
    "        model.loss(x_train_batches[batch], y_train_batches[batch]).backward()\n",
    "        optimizer.step()  # Perform optimization by adjusting W and b,\n",
    "        optimizer.zero_grad()  # Clear gradients for next step\n",
    "\n",
    "    print(f'Accuracy in epoch {i+1}: {model.accuracy(x_test, y_test)*100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy in epoch 1: 86.0999984741211%\nAccuracy in epoch 2: 88.11000061035156%\nAccuracy in epoch 3: 88.84000396728516%\nAccuracy in epoch 4: 89.56000518798828%\nAccuracy in epoch 5: 89.2300033569336%\nAccuracy in epoch 6: 89.22000122070312%\nAccuracy in epoch 7: 89.44000244140625%\nAccuracy in epoch 8: 89.74000549316406%\nAccuracy in epoch 9: 90.09000396728516%\nAccuracy in epoch 10: 90.29000091552734%\nAccuracy in epoch 11: 90.72000122070312%\nAccuracy in epoch 12: 90.66000366210938%\nAccuracy in epoch 13: 90.97999572753906%\nAccuracy in epoch 14: 90.54000091552734%\nAccuracy in epoch 15: 90.69999694824219%\nAccuracy in epoch 16: 90.4800033569336%\nAccuracy in epoch 17: 90.44999694824219%\nAccuracy in epoch 18: 90.80999755859375%\nAccuracy in epoch 19: 90.81999969482422%\nAccuracy in epoch 20: 90.69999694824219%\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The task given was to try another dataset called Fashion MNIST, and create the best possible accuracy\n",
    "I use the nn.py from our lecturer as a base, and works mostly inside the class, within the init and forward-method\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load observations from the mnist dataset. The observations are divided into a training set and a test set\n",
    "mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    './data', train=True, download=True)\n",
    "# torch.functional.nn.conv2d argument must include channels (1)\n",
    "x_train = mnist_train.data.reshape(-1, 1, 28, 28).float()\n",
    "# Create output tensor\n",
    "y_train = torch.zeros((mnist_train.targets.shape[0], 10))\n",
    "y_train[torch.arange(mnist_train.targets.shape[0]),\n",
    "        mnist_train.targets] = 1  # Populate output\n",
    "\n",
    "mnist_test = torchvision.datasets.FashionMNIST(\n",
    "    './data', train=False, download=True)\n",
    "# torch.functional.nn.conv2d argument must include channels (1)\n",
    "x_test = mnist_test.data.reshape(-1, 1, 28, 28).float()\n",
    "y_test = torch.zeros((mnist_test.targets.shape[0], 10))  # Create output tensor\n",
    "y_test[torch.arange(mnist_test.targets.shape[0]),\n",
    "       mnist_test.targets] = 1  # Populate output\n",
    "\n",
    "# Normalization of inputs\n",
    "mean = x_train.mean()\n",
    "std = x_train.std()\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "# Divide training data into batches to speed up optimization\n",
    "batches = 600\n",
    "x_train_batches = torch.split(x_train, batches)\n",
    "y_train_batches = torch.split(y_train, batches)\n",
    "\n",
    "\n",
    "class ConvolutionalNeuralNetworkModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNeuralNetworkModel, self).__init__()\n",
    "        # Initialises the Model layers (includes initialized model variables):\n",
    "        # Could also write stride=2 here, but I suspect it is the standard for kernelsize 2 (as 2x2 will jump 2 to the right for the next 2x2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.pool(self.conv1(x))\n",
    "        # x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.dropout(F.relu(self.conv1(x)), p=0.1))\n",
    "        \n",
    "        # x = self.pool(self.conv2(x))\n",
    "        # x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.dropout(F.relu(self.conv2(x)), p=0.1))\n",
    "        \n",
    "        x = self.fc1(x.view(-1, 64 * 7 * 7))\n",
    "        x = self.fc2(x.view(-1, 1024))\n",
    "        x = self.fc3(x.view(-1, 100))\n",
    "        return x\n",
    "\n",
    "    # Predictor\n",
    "    def f(self, x):\n",
    "        return torch.softmax(self.forward(x), dim=1)\n",
    "\n",
    "    # Cross Entropy loss\n",
    "    def loss(self, x, y):\n",
    "        return nn.functional.cross_entropy(self.forward(x), y.argmax(1))\n",
    "\n",
    "    # Accuracy\n",
    "    def accuracy(self, x, y):\n",
    "        return torch.mean(torch.eq(self.f(x).argmax(1), y.argmax(1)).float())\n",
    "\n",
    "\n",
    "model = ConvolutionalNeuralNetworkModel()\n",
    "\n",
    "epoch = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Optimize: adjust W and b to minimize loss using stochastic gradient descent\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for i, epoch in enumerate(range(epoch)):\n",
    "    for batch in range(len(x_train_batches)):\n",
    "        # Compute loss gradients\n",
    "        model.loss(x_train_batches[batch], y_train_batches[batch]).backward()\n",
    "        optimizer.step()  # Perform optimization by adjusting W and b,\n",
    "        optimizer.zero_grad()  # Clear gradients for next step\n",
    "\n",
    "    print(f'Accuracy in epoch {i+1}: {model.accuracy(x_test, y_test)*100}%')\n"
   ]
  }
 ]
}