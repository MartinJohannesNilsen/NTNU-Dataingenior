{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600425642660",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Oppgave A, f√∏rst med LSTM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\"\"\"\n",
    "In this task we are going to use the main.py given in the lecture and use it to print out the sentence 'hello world' instead of 'hello'\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LongShortTermMemoryModel(nn.Module):\n",
    "    def __init__(self, encoding_size):\n",
    "        super(LongShortTermMemoryModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(encoding_size, 128)  # 128 is the state size\n",
    "        self.fc1 = nn.Linear(128, encoding_size)  # 128 is the state size\n",
    "\n",
    "    def reset(self):  # Reset states prior to new input sequence\n",
    "        # Shape: (number of layers, batch size, state size)\n",
    "        zero_state = torch.zeros(1, 1, 128)\n",
    "        self.hidden_state = zero_state\n",
    "        self.cell_state = zero_state\n",
    "\n",
    "    def forward(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        out, (self.hidden_state, self.cell_state) = self.lstm(\n",
    "            x, (self.hidden_state, self.cell_state))\n",
    "        return self.fc1(out.reshape(-1, 128))\n",
    "\n",
    "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        return torch.softmax(self.forward(x), dim=1)\n",
    "\n",
    "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\n",
    "        return nn.functional.cross_entropy(self.forward(x), y.argmax(1))\n",
    "\n",
    "\n",
    "char_encodings = [\n",
    "    [1., 0., 0., 0., 0., 0., 0., 0.],  # ' '\n",
    "    [0., 1., 0., 0., 0., 0., 0., 0.],  # 'h'\n",
    "    [0., 0., 1., 0., 0., 0., 0., 0.],  # 'e'\n",
    "    [0., 0., 0., 1., 0., 0., 0., 0.],  # 'l'\n",
    "    [0., 0., 0., 0., 1., 0., 0., 0.],  # 'o'\n",
    "    [0., 0., 0., 0., 0., 1., 0., 0.],  # 'w'\n",
    "    [0., 0., 0., 0., 0., 0., 1., 0.],  # 'r'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 1.],  # 'd'\n",
    "]\n",
    "encoding_size = len(char_encodings)\n",
    "\n",
    "index_to_char = [' ', 'h', 'e', 'l', 'o', 'w', 'r', 'd']\n",
    "\n",
    "x_train = torch.tensor([[char_encodings[0]], [char_encodings[1]], [char_encodings[2]], [char_encodings[3]], [char_encodings[3]],\n",
    "                        [char_encodings[4]], [char_encodings[0]], [char_encodings[5]], [char_encodings[4]], [char_encodings[6]], [char_encodings[3]], [char_encodings[7]]])  # ' hello world'\n",
    "y_train = torch.tensor([char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[3], char_encodings[4], char_encodings[0],\n",
    "                        char_encodings[5], char_encodings[4], char_encodings[6], char_encodings[3], char_encodings[7], char_encodings[0]])  # 'hello world '\n",
    "\n",
    "model = LongShortTermMemoryModel(encoding_size)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), 0.001)\n",
    "for epoch in range(500):\n",
    "    model.reset()\n",
    "    model.loss(x_train, y_train).backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        # Generate characters from the initial characters ' h'\n",
    "        model.reset()\n",
    "        text = ' h'\n",
    "        model.f(torch.tensor([[char_encodings[0]]]))\n",
    "        y = model.f(torch.tensor([[char_encodings[1]]]))\n",
    "        text += index_to_char[y.argmax(1)]\n",
    "        for c in range(50):\n",
    "            y = model.f(torch.tensor([[char_encodings[y.argmax(1)]]]))\n",
    "            text += index_to_char[y.argmax(1)]\n",
    "        print(text)\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hlllo                                               \n hlllo wold   rld   lld   ll    ll    ll    ll    ll \n hello world   rlld  wrll   wrld  wrlld  wrll   rlld \n hello world  wrld  wrrld  wrld  wrlld  wrld  wrlld  \n hello world  wrld  wrrld  wrld  wrlld  wrld  wrlld w\n hello world  wrld  wrrld  wrld  wrlld world  wrlld w\n hello world  wrll  world  wrll  world  wrll  world  \n hello world  wrll  world  wrll  world  wrll  world  \n hello world  wrll  world world  wrlld world  wrlld w\n hello world world  wrlld world  wrlld world world  w\n hello world world  wrlld world world  wrlld world wo\n hello world world  wrlld world world  wrlld world wo\n hello world world  wrlld world world  wrlld world wo\n hello world world world  wrlld world world  wrlld wo\n hello world world world  wrlld world world  wrlld wo\n hello world world world  wrlld world world world  wr\n hello world world world  wrlld world world world  wr\n hello world world world world  wrlld world world wor\n hello world world world world  wrlld world world wor\n hello world world world world  wrlld world world wor\n hello world world world world world  wrlld world wor\n hello world world world world world  wrlld world wor\n hello world world world world world  hrlld world wor\n hello world world world world world world  wrlld wor\n hello world world world world world world  hrlld wor\n hello world world world world world world world  wrl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n"
    }
   ]
  },
  {
   "source": [
    "### Oppgave A, med RNN for sammenlikning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hello world world world wo lorlo world world world w\n hello world world world lo world world world  oolld\n hello world world world loolorlo world world  oollo \n hello world world world looldrlooworld  oollo world \n hello world world world  oollo world wo lorlo world \n hello world world world  oollo world world wo worlo \n hello world world world  ooldo world world wo ld lo \n hello world world world  ooldo world world world wo \n hello world world world world  oollo world world wo \n hello world world world world  oollo world world wor\n hello world world world world  oollo world world wor\n hello world world world world  oollo world world wor\n hello world world world world  oollo world world wor\n hello world world world world  oollo world world wor\n hello world world world world  oollo world world wor\n hello world world world world  oollo world world wor\n hello world world world world  oollo world world wor\n hello world world world world  oollo world world wor\n hello world world world world  oollo world world wor\n hello world world world world  ooldo world world wor\n hello world world world world world  oollo world wor\n hello world world world world world  oollo world wor\n hello world world world world world  oollo world wor\n hello world world world world world  oollo world wor\n hello world world world world world  oollo world wor\n hello world world world world world  oollo world wor\n hello world world world world world  oollo world wor\n hello world world world world world  oollo world wor\n hello world world world world world world  oollo wor\n hello world world world world world world  oollo wor\n hello world world world world world world  oollo wor\n hello world world world world world world  oollo wor\n hello world world world world world world  oollo wor\n hello world world world world world world  oollo wor\n hello world world world world world world world  ool\n hello world world world world world world world  ool\n hello world world world world world world world  ool\n hello world world world world world world world  ool\n hello world world world world world world world  orl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n hello world world world world world world world worl\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this task we are going to use the main.py given in the lecture and use it to print out the sentence 'hello world' instead of 'hello'\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LongShortTermMemoryModel(nn.Module):\n",
    "    def __init__(self, encoding_size):\n",
    "        super(LongShortTermMemoryModel, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(encoding_size, 128)  # 128 is the state size\n",
    "        self.fc1 = nn.Linear(128, encoding_size)  # 128 is the state size\n",
    "\n",
    "    def reset(self):  # Reset states prior to new input sequence\n",
    "        # Shape: (number of layers, batch size, state size)\n",
    "        zero_state = torch.zeros(1, 1, 128)\n",
    "        self.hidden_state = zero_state\n",
    "        self.cell_state = zero_state\n",
    "\n",
    "    def forward(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        out, (self.hidden_state) = self.rnn(\n",
    "            x, (self.hidden_state))\n",
    "        return self.fc1(out.reshape(-1, 128))\n",
    "\n",
    "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        return torch.softmax(self.forward(x), dim=1)\n",
    "\n",
    "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\n",
    "        return nn.functional.cross_entropy(self.forward(x), y.argmax(1))\n",
    "\n",
    "\n",
    "char_encodings = [\n",
    "    [1., 0., 0., 0., 0., 0., 0., 0.],  # ' '\n",
    "    [0., 1., 0., 0., 0., 0., 0., 0.],  # 'h'\n",
    "    [0., 0., 1., 0., 0., 0., 0., 0.],  # 'e'\n",
    "    [0., 0., 0., 1., 0., 0., 0., 0.],  # 'l'\n",
    "    [0., 0., 0., 0., 1., 0., 0., 0.],  # 'o'\n",
    "    [0., 0., 0., 0., 0., 1., 0., 0.],  # 'w'\n",
    "    [0., 0., 0., 0., 0., 0., 1., 0.],  # 'r'\n",
    "    [0., 0., 0., 0., 0., 0., 0., 1.],  # 'd'\n",
    "]\n",
    "encoding_size = len(char_encodings)\n",
    "\n",
    "index_to_char = [' ', 'h', 'e', 'l', 'o', 'w', 'r', 'd']\n",
    "\n",
    "x_train = torch.tensor([[char_encodings[0]], [char_encodings[1]], [char_encodings[2]], [char_encodings[3]], [char_encodings[3]],\n",
    "                        [char_encodings[4]], [char_encodings[0]], [char_encodings[5]], [char_encodings[4]], [char_encodings[6]], [char_encodings[3]], [char_encodings[7]]])  # ' hello world'\n",
    "y_train = torch.tensor([char_encodings[1], char_encodings[2], char_encodings[3], char_encodings[3], char_encodings[4], char_encodings[0],\n",
    "                        char_encodings[5], char_encodings[4], char_encodings[6], char_encodings[3], char_encodings[7], char_encodings[0]])  # 'hello world '\n",
    "\n",
    "model = LongShortTermMemoryModel(encoding_size)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), 0.001)\n",
    "for epoch in range(500):\n",
    "    model.reset()\n",
    "    model.loss(x_train, y_train).backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        # Generate characters from the initial characters ' h'\n",
    "        model.reset()\n",
    "        text = ' h'\n",
    "        model.f(torch.tensor([[char_encodings[0]]]))\n",
    "        y = model.f(torch.tensor([[char_encodings[1]]]))\n",
    "        text += index_to_char[y.argmax(1)]\n",
    "        for c in range(50):\n",
    "            y = model.f(torch.tensor([[char_encodings[y.argmax(1)]]]))\n",
    "            text += index_to_char[y.argmax(1)]\n",
    "        print(text)\n"
   ]
  },
  {
   "source": [
    "### Oppgave B, med \"rt \""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The word is \"rt \":\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this task we are going to use the main.py given in the lecture and use it to print out emojies based on the words\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import sys\n",
    "\n",
    "\n",
    "class LongShortTermMemoryModel(nn.Module):\n",
    "    def __init__(self, encoding_size, label_size):\n",
    "        super(LongShortTermMemoryModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(char_encoding_size, 128)  # 128 is the state size\n",
    "        self.fc1 = nn.Linear(128, emoji_encoding_size)\n",
    "\n",
    "    # Batch size is the number of sequences to be passed to the LSTM.\n",
    "    # When training, batch size is typically > 1, but the batch size is 1 when generating\n",
    "    def reset(self, batch_size=1):  # Reset states prior to new input sequence\n",
    "        # Shape: (number of layers, batch size, state size)\n",
    "        zero_state = torch.zeros(1, batch_size, 128)\n",
    "        self.hidden_state = zero_state\n",
    "        self.cell_state = zero_state\n",
    "\n",
    "    def logits(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        out, (self.hidden_state, self.cell_state) = self.lstm(\n",
    "            x, (self.hidden_state, self.cell_state))\n",
    "        return self.fc1(out.reshape(-1, 128))\n",
    "\n",
    "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        return torch.softmax(self.logits(x), dim=1)\n",
    "\n",
    "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\n",
    "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    SEARCH_WORD = 'rt '\n",
    "\n",
    "    # Tried to make it possible to run with search word in command, got weird import error\n",
    "    \"\"\" \n",
    "    if len(sys.argv) < 2:  # If argument is not given\n",
    "        print(\"Hint: Run $Python string_to_emoji.py \\'<Search Word>\\'\")\n",
    "    else:\n",
    "        SEARCH_WORD = sys.argv[1]\n",
    "    \"\"\"\n",
    "    print(\"The word is \\\"{}\\\":\".format(SEARCH_WORD))\n",
    "\n",
    "    # Input in text form\n",
    "    char_enc = [\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # ' ' 0\n",
    "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'h' 1\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'a' 2\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 't' 3\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'r' 4\n",
    "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],  # 'c' 5\n",
    "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],  # 'f' 6\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],  # 'l' 7\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],  # 'm' 8\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],  # 'p' 9\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],  # 's' 10\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],  # 'o' 11\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]   # 'n' 12\n",
    "    ]\n",
    "    char_encoding_size = len(char_enc)\n",
    "    index_to_char = [' ', 'h', 'a', 't', 'r',\n",
    "                     'c', 'f', 'l', 'm', 'p', 's', 'o', 'n']\n",
    "\n",
    "    # Output\n",
    "    emojis = {\n",
    "        'hat': '\\U0001F3A9',\n",
    "        'cat': '\\U0001F408',\n",
    "        'rat': '\\U0001F400',\n",
    "        'flat': '\\U0001F3E2',\n",
    "        'matt': '\\U0001F468',\n",
    "        'cap': '\\U0001F9E2',\n",
    "        'son': '\\U0001F466'\n",
    "    }\n",
    "\n",
    "    emoji_enc = [\n",
    "        [1., 0., 0., 0., 0., 0., 0.],  # 'hat' 0\n",
    "        [0., 1., 0., 0., 0., 0., 0.],  # 'rat' 1\n",
    "        [0., 0., 1., 0., 0., 0., 0.],  # 'cat' 2\n",
    "        [0., 0., 0., 1., 0., 0., 0.],  # 'flat' 3\n",
    "        [0., 0., 0., 0., 1., 0., 0.],  # 'matt' 4\n",
    "        [0., 0., 0., 0., 0., 1., 0.],  # 'cap' 5\n",
    "        [0., 0., 0., 0., 0., 0., 1.]   # 'son' 6\n",
    "    ]\n",
    "    emoji_encoding_size = len(emoji_enc)\n",
    "    index_to_emoji = [emojis['hat'], emojis['rat'], emojis['cat'],\n",
    "                      emojis['flat'], emojis['matt'], emojis['cap'], emojis['son']]\n",
    "\n",
    "    x_train = torch.tensor([\n",
    "        [[char_enc[1]], [char_enc[2]], [char_enc[3]], [char_enc[0]]],\n",
    "        [[char_enc[4]], [char_enc[2]], [char_enc[3]], [char_enc[0]]],\n",
    "        [[char_enc[5]], [char_enc[2]], [char_enc[3]], [char_enc[0]]],\n",
    "        [[char_enc[6]], [char_enc[7]], [char_enc[2]], [char_enc[3]]],\n",
    "        [[char_enc[8]], [char_enc[2]], [char_enc[3]], [char_enc[3]]],\n",
    "        [[char_enc[5]], [char_enc[2]], [char_enc[9]], [char_enc[0]]],\n",
    "        [[char_enc[10]], [char_enc[11]], [char_enc[12]], [char_enc[0]]]])  # 'hat ', 'rat ', 'cat ', 'flat', 'matt', 'cap ', 'son '\n",
    "\n",
    "    y_train = torch.tensor([\n",
    "        [emoji_enc[0], emoji_enc[0], emoji_enc[0], emoji_enc[0]],\n",
    "        [emoji_enc[1], emoji_enc[1], emoji_enc[1], emoji_enc[1]],\n",
    "        [emoji_enc[2], emoji_enc[2], emoji_enc[2], emoji_enc[2]],\n",
    "        [emoji_enc[3], emoji_enc[3], emoji_enc[3], emoji_enc[3]],\n",
    "        [emoji_enc[4], emoji_enc[4], emoji_enc[4], emoji_enc[4]],\n",
    "        [emoji_enc[5], emoji_enc[5], emoji_enc[5], emoji_enc[5]],\n",
    "        [emoji_enc[6], emoji_enc[6], emoji_enc[6], emoji_enc[6]]])\n",
    "\n",
    "    model = LongShortTermMemoryModel(char_encoding_size, emoji_encoding_size)\n",
    "\n",
    "    def generate(string):\n",
    "        model.reset()\n",
    "        for i in range(len(string)):\n",
    "            char_index = index_to_char.index(string[i])\n",
    "            y = model.f(torch.tensor([[char_enc[char_index]]]))\n",
    "            if i == len(string) - 1:\n",
    "                print(index_to_emoji[y.argmax(1)])\n",
    "\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), 0.001)  # 0.001\n",
    "    for epoch in range(500):\n",
    "        for i in range(x_train.size()[0]):\n",
    "            model.reset()\n",
    "            model.loss(x_train[i], y_train[i]).backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if epoch % 10 == 9:\n",
    "            generate(SEARCH_WORD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Oppgave B, med \"rats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The word is \"rats\":\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\nüêÄ\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this task we are going to use the main.py given in the lecture and use it to print out emojies based on the words\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import sys\n",
    "\n",
    "\n",
    "class LongShortTermMemoryModel(nn.Module):\n",
    "    def __init__(self, encoding_size, label_size):\n",
    "        super(LongShortTermMemoryModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(char_encoding_size, 128)  # 128 is the state size\n",
    "        self.fc1 = nn.Linear(128, emoji_encoding_size)\n",
    "\n",
    "    # Batch size is the number of sequences to be passed to the LSTM.\n",
    "    # When training, batch size is typically > 1, but the batch size is 1 when generating\n",
    "    def reset(self, batch_size=1):  # Reset states prior to new input sequence\n",
    "        # Shape: (number of layers, batch size, state size)\n",
    "        zero_state = torch.zeros(1, batch_size, 128)\n",
    "        self.hidden_state = zero_state\n",
    "        self.cell_state = zero_state\n",
    "\n",
    "    def logits(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        out, (self.hidden_state, self.cell_state) = self.lstm(\n",
    "            x, (self.hidden_state, self.cell_state))\n",
    "        return self.fc1(out.reshape(-1, 128))\n",
    "\n",
    "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        return torch.softmax(self.logits(x), dim=1)\n",
    "\n",
    "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\n",
    "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    SEARCH_WORD = 'rats'\n",
    "\n",
    "    # Tried to make it possible to run with search word in command, got weird import error\n",
    "    \"\"\" \n",
    "    if len(sys.argv) < 2:  # If argument is not given\n",
    "        print(\"Hint: Run $Python string_to_emoji.py \\'<Search Word>\\'\")\n",
    "    else:\n",
    "        SEARCH_WORD = sys.argv[1]\n",
    "    \"\"\"\n",
    "    print(\"The word is \\\"{}\\\":\".format(SEARCH_WORD))\n",
    "\n",
    "    # Input in text form\n",
    "    char_enc = [\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # ' ' 0\n",
    "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'h' 1\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'a' 2\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],  # 't' 3\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],  # 'r' 4\n",
    "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],  # 'c' 5\n",
    "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],  # 'f' 6\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],  # 'l' 7\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],  # 'm' 8\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],  # 'p' 9\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],  # 's' 10\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],  # 'o' 11\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]   # 'n' 12\n",
    "    ]\n",
    "    char_encoding_size = len(char_enc)\n",
    "    index_to_char = [' ', 'h', 'a', 't', 'r',\n",
    "                     'c', 'f', 'l', 'm', 'p', 's', 'o', 'n']\n",
    "\n",
    "    # Output\n",
    "    emojis = {\n",
    "        'hat': '\\U0001F3A9',\n",
    "        'cat': '\\U0001F408',\n",
    "        'rat': '\\U0001F400',\n",
    "        'flat': '\\U0001F3E2',\n",
    "        'matt': '\\U0001F468',\n",
    "        'cap': '\\U0001F9E2',\n",
    "        'son': '\\U0001F466'\n",
    "    }\n",
    "\n",
    "    emoji_enc = [\n",
    "        [1., 0., 0., 0., 0., 0., 0.],  # 'hat' 0\n",
    "        [0., 1., 0., 0., 0., 0., 0.],  # 'rat' 1\n",
    "        [0., 0., 1., 0., 0., 0., 0.],  # 'cat' 2\n",
    "        [0., 0., 0., 1., 0., 0., 0.],  # 'flat' 3\n",
    "        [0., 0., 0., 0., 1., 0., 0.],  # 'matt' 4\n",
    "        [0., 0., 0., 0., 0., 1., 0.],  # 'cap' 5\n",
    "        [0., 0., 0., 0., 0., 0., 1.]   # 'son' 6\n",
    "    ]\n",
    "    emoji_encoding_size = len(emoji_enc)\n",
    "    index_to_emoji = [emojis['hat'], emojis['rat'], emojis['cat'],\n",
    "                      emojis['flat'], emojis['matt'], emojis['cap'], emojis['son']]\n",
    "\n",
    "    x_train = torch.tensor([\n",
    "        [[char_enc[1]], [char_enc[2]], [char_enc[3]], [char_enc[0]]],\n",
    "        [[char_enc[4]], [char_enc[2]], [char_enc[3]], [char_enc[0]]],\n",
    "        [[char_enc[5]], [char_enc[2]], [char_enc[3]], [char_enc[0]]],\n",
    "        [[char_enc[6]], [char_enc[7]], [char_enc[2]], [char_enc[3]]],\n",
    "        [[char_enc[8]], [char_enc[2]], [char_enc[3]], [char_enc[3]]],\n",
    "        [[char_enc[5]], [char_enc[2]], [char_enc[9]], [char_enc[0]]],\n",
    "        [[char_enc[10]], [char_enc[11]], [char_enc[12]], [char_enc[0]]]])  # 'hat ', 'rat ', 'cat ', 'flat', 'matt', 'cap ', 'son '\n",
    "\n",
    "    y_train = torch.tensor([\n",
    "        [emoji_enc[0], emoji_enc[0], emoji_enc[0], emoji_enc[0]],\n",
    "        [emoji_enc[1], emoji_enc[1], emoji_enc[1], emoji_enc[1]],\n",
    "        [emoji_enc[2], emoji_enc[2], emoji_enc[2], emoji_enc[2]],\n",
    "        [emoji_enc[3], emoji_enc[3], emoji_enc[3], emoji_enc[3]],\n",
    "        [emoji_enc[4], emoji_enc[4], emoji_enc[4], emoji_enc[4]],\n",
    "        [emoji_enc[5], emoji_enc[5], emoji_enc[5], emoji_enc[5]],\n",
    "        [emoji_enc[6], emoji_enc[6], emoji_enc[6], emoji_enc[6]]])\n",
    "\n",
    "    model = LongShortTermMemoryModel(char_encoding_size, emoji_encoding_size)\n",
    "\n",
    "    def generate(string):\n",
    "        model.reset()\n",
    "        for i in range(len(string)):\n",
    "            char_index = index_to_char.index(string[i])\n",
    "            y = model.f(torch.tensor([[char_enc[char_index]]]))\n",
    "            if i == len(string) - 1:\n",
    "                print(index_to_emoji[y.argmax(1)])\n",
    "\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), 0.001)  # 0.001\n",
    "    for epoch in range(500):\n",
    "        for i in range(x_train.size()[0]):\n",
    "            model.reset()\n",
    "            model.loss(x_train[i], y_train[i]).backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if epoch % 10 == 9:\n",
    "            generate(SEARCH_WORD)\n"
   ]
  }
 ]
}